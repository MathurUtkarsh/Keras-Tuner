{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is hyperparameter tuning?\n",
    "\n",
    "\n",
    "A machine learning model has two types of parameters:\n",
    "\n",
    "**Trainable parameters**, which are learned by the algorithm during training. For instance, the weights of a neural network are trainable parameters.\n",
    "\n",
    "**Hyperparameters**, which need to be set before launching the learning process. The learning rate or the number of units in a dense layer are hyperparameters.\n",
    "\n",
    "Hyperparameters can be numerous even for small models. Tuning them can be a real brain teaser but worth the challenge: a good hyperparameter combination can highly improve your model's performance. Here we'll see that on a simple CNN model, it can help you gain 10% accuracy on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with Keras Tuner\n",
    "\n",
    "\n",
    "![tuner_image](https://images.ctfassets.net/be04ylp8y0qc/1m1AB8NPTcKkuEaqg2Zgyg/e56d45a2c85b99820ead8159a280323f/hp_tuning_flow_1f745fd5e0ae8830804bab6a66e2c917_1000.png?fm=webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does Kera-Tuner work?\n",
    "\n",
    "First, a tuner is defined. Its role is to determine which hyperparameter combinations should be tested. The library search function performs the iteration loop, which evaluates a certain number of hyperparameter combinations. Evaluation is performed by computing the trained model's accuracy on a held-out validation set.\n",
    "\n",
    "Finally, the best hyperparameter combination in terms of validation accuracy can be tested on a held-out test set.\n",
    "\n",
    "#### Building end-to-end pipeline to tune a simple convolutional network's hyperparameters for object classification on the CIFAR10 dataset.\n",
    "\n",
    "**Install Keras Tuner**\n",
    "\n",
    "    pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the **CIFAR10 dataset**. CIFAR10 is a common benchmarking dataset in computer vision. It contains 10 classes and is relatively small, with 60000 images. This size allows for a relatively short training time which we'll take advantage of to perform multiple hyperparameter tuning iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# Pre-processing\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The tuner expects floats as inputs, and the division by 255 is a data normalization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model definition\n",
    "\n",
    "Here, we will work with a simple convolutional model to classify each image into one of the 10 available classes.\n",
    "\n",
    "![arch](https://images.ctfassets.net/be04ylp8y0qc/1vDpd4pZYAcSLrN5lr525T/eb3a5873f1a2e4eb0df72bbae1f74448/simple_cnn_c0a8b6073d06e3c72809af2d91fd4cf9_800.jpeg?fm=webp)\n",
    "\n",
    "Each input image will go through two convolutional blocks (2 convolution layers followed by a pooling layer) and a dropout layer for regularization purposes. Finally, each output is flattened and goes through a dense layer that classify the image into one of the 10 classes.\n",
    "\n",
    "In Keras, this model can be defined as below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling2D\n",
    ")\n",
    "\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        input_shape=INPUT_SHAPE\n",
    "    )\n",
    ")\n",
    "model.add(Conv2D(16, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(32, 3, activation='relu'))\n",
    "model.add(Conv2D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search Space definition\n",
    "\n",
    "To perform hyperparameter tuning, we need to define the search space, that is to say which hyperparameters need to be optimized and in what range. Here, for this relatively small model, there are already 6 hyperparameters that can be tuned:\n",
    "\n",
    "the dropout rate for the three dropout layers\n",
    "\n",
    "the number of filters for the convolutional layers\n",
    "\n",
    "the number of units for the dense layer\n",
    "\n",
    "its activation function\n",
    "\n",
    "In Keras Tuner, hyperparameters have a type (possibilities are Float, Int, Boolean, and Choice) and a unique name. Then, a set of options to help guide the search need to be set:\n",
    "\n",
    "a minimal, a maximal and a default value for the Float and the Int types\n",
    "\n",
    "a set of possible values for the Choice type\n",
    "\n",
    "optionally, a sampling method within linear, log or reversed log. Setting this parameter allows to add prior knowledge you might have about the tuned parameter. We'll see in the next section how it can be used to tune the learning rate for instance\n",
    "\n",
    "optionally, a step value, i.e the minimal step between two hyperparameter values\n",
    "\n",
    "For instance, to set the hyperparameter 'number of filters' you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filters=hp.Choice(\n",
    "    'num_filters',\n",
    "    values=[32, 64],\n",
    "    default=64,\n",
    "),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The dense layer has two hyperparameters, the number of units and the activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Dense(\n",
    "    units=hp.Int(\n",
    "        'units',\n",
    "        min_value=32,\n",
    "        max_value=512,\n",
    "        step=32,\n",
    "        default=128\n",
    "    ),\n",
    "    activation=hp.Choice(\n",
    "        'dense_activation',\n",
    "        values=['relu', 'tanh', 'sigmoid'],\n",
    "        default='relu'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation\n",
    "\n",
    "Then let's move to model compilation, where other hyperparameters are also present. The compilation step is where the optimizer along with the loss function and the metric are defined. Here, we'll use categorical entropy as a loss function and accuracy as a metric. For the optimizer, different options are available. We'll use the popular Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the learning rate, which represents how fast the learning algorithm progresses, is often an important hyperparameter. Usually, the learning rate is chosen on a log scale. This prior knowledge can be incorporated in the search through the setting of the sampling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hp.Float(\n",
    "    'learning_rate',\n",
    "    min_value=1e-5,\n",
    "    max_value=1e-2,\n",
    "    sampling='LOG',\n",
    "    default=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tuner Hypermodels\n",
    "\n",
    "To put the whole hyperparameter search space together and perform hyperparameter tuning, Keras Tuners uses `HyperModel` instances. Hypermodels are reusable class object introduced with the library, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "\n",
    "\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=16,\n",
    "                kernel_size=3,\n",
    "                activation='relu',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=16,\n",
    "                activation='relu',\n",
    "                kernel_size=3\n",
    "            )\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(\n",
    "            Dropout(rate=hp.Float(\n",
    "                'dropout_1',\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.05,\n",
    "            ))\n",
    "        )\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=3,\n",
    "                activation='relu'\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=hp.Choice(\n",
    "                    'num_filters',\n",
    "                    values=[32, 64],\n",
    "                    default=64,\n",
    "                ),\n",
    "                activation='relu',\n",
    "                kernel_size=3\n",
    "            )\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(\n",
    "            Dropout(rate=hp.Float(\n",
    "                'dropout_2',\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.05,\n",
    "            ))\n",
    "        )\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(\n",
    "                    'units',\n",
    "                    min_value=32,\n",
    "                    max_value=512,\n",
    "                    step=32,\n",
    "                    default=128\n",
    "                ),\n",
    "                activation=hp.Choice(\n",
    "                    'dense_activation',\n",
    "                    values=['relu', 'tanh', 'sigmoid'],\n",
    "                    default='relu'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Dropout(\n",
    "                rate=hp.Float(\n",
    "                    'dropout_3',\n",
    "                    min_value=0.0,\n",
    "                    max_value=0.5,\n",
    "                    default=0.25,\n",
    "                    step=0.05\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling='LOG',\n",
    "                    default=1e-3\n",
    "                )\n",
    "            ),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "hypermodel = CNNHyperModel(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The library already offers two on-the-shelf hypermodels for computer vision, HyperResNet and HyperXception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the tuner\n",
    "\n",
    "Keras Tuner offers the main hyperparameter tuning methods: random search, Hyperband, and Bayesian optimization.\n",
    "\n",
    "In this tutorial, we'll focus on random search and Hyperband. We won't go into theory, but if you want to know more about random search and Bayesian Optimization, I wrote a post about it: Bayesian optimization for hyperparameter tuning. As for Hyperband, its main idea is to optimize Random Search in terms of search time.\n",
    "\n",
    "For every tuner, a seed parameter can be defined for experiments reproducibility: SEED = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "The most intuitive way to perform hyperparameter tuning is to randomly sample hyperparameter combinations and test them out. This is exactly what the RandomSearch tuner does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "NUM_CLASSES = 10  # cifar10 number of classes\n",
    "INPUT_SHAPE = (32, 32, 3)  # cifar10 images input shape\n",
    "\n",
    "hypermodel = CNNHyperModel(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    seed=SEED,\n",
    "    max_trials=MAX_TRIALS,\n",
    "    executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "    directory='random_search',\n",
    "    project_name='cifar10'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is the function to optimize. The tuner infers if it is a maximization or a minimization problem based on its value.\n",
    "\n",
    "Then, the max_trials variable represents the number of hyperparameter combinations that will be tested by the tuner, while the execution_per_trial variable is the number of models that should be built and fit for each trial for robustness purposes. The next section explains how to set them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "\n",
    "tuner = Hyperband(\n",
    "    hypermodel,\n",
    "    max_epochs=HYPERBAND_MAX_EPOCHS,\n",
    "    objective='val_accuracy',\n",
    "    seed=SEED,\n",
    "    executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "    directory='hyperband',\n",
    "    project_name='cifar10'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperband is an optimized version of random search which uses early-stopping to speed up the hyperparameter tuning process. The main idea is to fit a large number of models for a small number of epochs and to only continue training for the models achieving the highest accuracy on the validation set. The max_epochs variable is the max number of epochs that a model can be trained for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is slightly different than the determination of hyperparameters. **Indeed, these settings here will mostly depend on your computing time and resources**. The highest number of trials you can perform, the better! Regarding the number of epochs, it is best if you know how many epochs your model needs to converge. You can also use early-stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Once the model and the tuner are set up, a summary of the task is easily available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tuner](https://images.ctfassets.net/be04ylp8y0qc/RTQVnnpte1eH6A6kYjZnI/ba934cba168fd193c88daae416301d32/search_space_summary_85ac4bdeb6c76e0b63af13fa9aa69199_800.png?fm=webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning can start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH_SEARCH = 40\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=N_EPOCH_SEARCH, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The search function takes as input the training data and a validation split to perform hyperparameter combinations evaluation. The epochs parameter is used in random search and Bayesian Optimization to define the number of training epochs for each hyperparameter combination.\n",
    "\n",
    "Finally, the search results can be summarized and used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show a summary of the search\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model.\n",
    "loss, accuracy = best_model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results were obtained :-\n",
    "    \n",
    "![img](https://images.ctfassets.net/be04ylp8y0qc/7LUFbWdFpTMzMF5vOXEDem/e11364fbea6c0070e43b60fd73bf3dae/tuning_results_f60a58eb6a16fe496b4819d75a7ed838_800.png?fm=webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are far from the 99.3% accuracy achieved by state-of-the-art models on the CIFAR10 dataset but not so bad for such a simple network structure. You can already see notable improvement between the baselines and the tuned models, with a boost of more than 10% in accuracy between Random Search and the first baseline.\n",
    "\n",
    "Overall, the Keras Tuner library is a nice and easy to learn option to perform hyperparameter tuning for your Keras and Tensorflow 2.O models. **The main step is you have to work on is adapting your model to fit the hypermodel format**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
